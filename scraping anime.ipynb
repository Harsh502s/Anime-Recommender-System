{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Anime from Zoro.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page_urls = []\n",
    "\n",
    "for i in range(1,170):\n",
    "    if i == 1:\n",
    "        page_urls.append('https://aniwatch.to/az-list/')\n",
    "    else:\n",
    "        page_urls.append('https://aniwatch.to/az-list/?page=' + str(i) + '/')\n",
    "        pass\n",
    "    pass\n",
    "page_urls\n",
    "# Scraping the data from all the pages\n",
    "\n",
    "anime_urls = []\n",
    "\n",
    "for url in page_urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Getting the url for the anime page\n",
    "    \n",
    "    for anime in soup.find_all('div',class_='film-poster'):\n",
    "        anime = anime.find('a')['href']\n",
    "        page = 'https://aniwatch.to'+ anime\n",
    "        anime_urls.append(page)\n",
    "        pass\n",
    "    \n",
    "\n",
    "    pass\n",
    "anime_dict = {'anime_urls':anime_urls}\n",
    "anime_df = pd.DataFrame(anime_dict)\n",
    "\n",
    "anime_df.to_csv('anime_urls.csv',index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the data from all the anime pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings as w\n",
    "w.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('anime_urls.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(df):\n",
    "\n",
    "    anime_poster = []\n",
    "    anime_title = []\n",
    "    anime_overview = []\n",
    "    anime_mal_score = []\n",
    "    anime_views = []\n",
    "    anime_genres = []\n",
    "    anime_studio = []\n",
    "    anime_producer = []\n",
    "\n",
    "    for url in df['anime_urls']:\n",
    "        \n",
    "        soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "        \n",
    "        # Getting the poster of the anime (url)\n",
    "        \n",
    "        anime_poster.append(soup.find('div',class_='film-poster').find('img')['src'])\n",
    "\n",
    "        # Getting the name of the anime\n",
    "        \n",
    "        anime_title.append(soup.find('h2',class_='film-name dynamic-name').text)\n",
    "        \n",
    "        # Getting the overview of the anime\n",
    "        \n",
    "        anime_overview.append(soup.find('div',class_='item item-title w-hide').find('div',class_='text').text)\n",
    "        \n",
    "        # Creating an object of the div containing all the details of the anime\n",
    "        \n",
    "        soup = soup.find('div',class_='anisc-info')\n",
    "\n",
    "        # Extract MAL Score\n",
    "        mal_score_element = soup.find('span', {'class': 'item-head'}, text='MAL Score:')\n",
    "        mal_score = mal_score_element.find_next_sibling('span', {'class': 'name'}).text.strip() if mal_score_element else 'NA'\n",
    "\n",
    "        # Extract Views\n",
    "        views_element = soup.find('span', {'class': 'item-head'}, text='Views:')\n",
    "        views = views_element.find_next_sibling('span', {'class': 'name'}).text.strip() if views_element else 'NA'\n",
    "\n",
    "        # Extract Studios\n",
    "        studios_element = soup.find('span', {'class': 'item-head'}, text='Studios:')\n",
    "        studios = studios_element.find_next('a', {'class': 'name'}).text.strip() if studios_element else 'NA'\n",
    "\n",
    "        # Extract Producers\n",
    "        producers_element = soup.find('span', {'class': 'item-head'}, text='Producers:')\n",
    "        producers = [producer.text.strip() for producer in producers_element.find_next_siblings('a')] if producers_element else ['NA']\n",
    "\n",
    "        # Extract Genres\n",
    "        genres_element = soup.find('span', {'class': 'item-head'}, text='Genres:')\n",
    "        genres = [genre.text.strip() for genre in genres_element.find_next_siblings('a')] if genres_element else ['NA']\n",
    "\n",
    "        anime_mal_score.append(mal_score)\n",
    "        anime_views.append(views)\n",
    "        anime_studio.append(studios)\n",
    "        anime_producer.append(producers)\n",
    "        anime_genres.append(genres)\n",
    "        pass\n",
    "    \n",
    "    anime_dict = {'anime_poster':anime_poster,'anime_title':anime_title,'anime_overview':anime_overview,'anime_mal_score':anime_mal_score,'anime_views':anime_views,'anime_studio':anime_studio,'anime_producer':anime_producer,'anime_genres':anime_genres}\n",
    "    anime_df = pd.DataFrame(anime_dict)\n",
    "    return anime_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = create_df(df)\n",
    "anime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.to_csv('anime_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
